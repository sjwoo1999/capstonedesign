import 'dart:async';
import 'dart:convert';
import 'dart:math';
import 'package:flutter_dotenv/flutter_dotenv.dart';
import 'package:http/http.dart' as http;
import '../models/multimodal_data_point.dart';
import 'emotion_api_services.dart';

/// ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤
/// ì˜ìƒ, ìŒì„±, í…ìŠ¤íŠ¸ ì„¸ ê°€ì§€ ëª¨ë‹¬ë¦¬í‹°ë¥¼ í†µí•©í•˜ì—¬ ë¶„ì„
class MultimodalAnalysisService {
  static String _baseUrl = dotenv.env['EMOTION_API_URL'] ?? 'http://localhost:5001';
  final EmotionAPIService _emotionApiService;
  
  MultimodalAnalysisService({
    EmotionAPIService? emotionApiService,
  }) : _emotionApiService = emotionApiService ?? EmotionAPIService();

  /// ğŸ”§ ë™ì ìœ¼ë¡œ BaseURL ì„¤ì • ê°€ëŠ¥
  static void setBaseUrl(String url) {
    _baseUrl = url;
    print('ğŸ“¡ Multimodal API ì„œë²„ ì£¼ì†Œ ì„¤ì •ë¨: $_baseUrl');
  }

  /// ì™„ì „í•œ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ì˜ìƒ + ìŒì„± + í…ìŠ¤íŠ¸)
  Future<MultimodalDataPoint> analyzeMultimodal({
    String? base64Image,
    String? base64Audio,
    String? text,
    String? sessionId,
    Map<String, dynamic>? metadata,
  }) async {
    print('ğŸš€ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì‹œì‘');
    print('ğŸ“Š ì…ë ¥ ë°ì´í„°: ì˜ìƒ=${base64Image != null ? "ìˆìŒ" : "ì—†ìŒ"}, ìŒì„±=${base64Audio != null ? "ìˆìŒ" : "ì—†ìŒ"}, í…ìŠ¤íŠ¸=${text != null ? "ìˆìŒ" : "ì—†ìŒ"}');

    final timestamp = DateTime.now();
    ModalityData? visualData;
    ModalityData? audioData;
    ModalityData? textData;

    // ë³‘ë ¬ë¡œ ê° ëª¨ë‹¬ë¦¬í‹° ë¶„ì„ ì‹¤í–‰
    final futures = <Future<void>>[];

    // 1. ì˜ìƒ ë¶„ì„
    if (base64Image != null && base64Image.isNotEmpty) {
      futures.add(_analyzeVisual(base64Image).then((data) => visualData = data));
    }

    // 2. ìŒì„± ë¶„ì„
    if (base64Audio != null && base64Audio.isNotEmpty) {
      futures.add(_analyzeAudio(base64Audio).then((data) => audioData = data));
    }

    // 3. í…ìŠ¤íŠ¸ ë¶„ì„
    if (text != null && text.isNotEmpty) {
      futures.add(_analyzeText(text).then((data) => textData = data));
    }

    // ëª¨ë“  ë¶„ì„ ì™„ë£Œ ëŒ€ê¸°
    if (futures.isNotEmpty) {
      await Future.wait(futures);
    }

    // í†µí•©ëœ ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
    final multimodalData = MultimodalDataPoint.fromModalities(
      timestamp: timestamp,
      visualData: visualData,
      audioData: audioData,
      textData: textData,
      sessionId: sessionId,
      metadata: metadata,
    );

    print('âœ… ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì™„ë£Œ: ${multimodalData.availableModalities}ê°œ ëª¨ë‹¬ë¦¬í‹°, ìµœì¢…ê°ì •: ${multimodalData.finalEmotion}');
    return multimodalData;
  }

  /// ì˜ìƒ ë¶„ì„
  Future<ModalityData?> _analyzeVisual(String base64Image) async {
    try {
      print('ğŸ“· ì˜ìƒ ë¶„ì„ ì‹œì‘');
      final response = await _emotionApiService.sendImageForAnalysis(base64Image);
      
      if (response.containsKey('face_emotion') && response.containsKey('final_vad')) {
        final vad = response['final_vad'] as Map<String, dynamic>;
        return ModalityData(
          valence: vad['valence']?.toDouble(),
          arousal: vad['arousal']?.toDouble(),
          dominance: vad['dominance']?.toDouble(),
          emotion: response['face_emotion'],
          confidence: response['confidence']?.toDouble() ?? 0.8,
          rawData: base64Image,
        );
      }
    } catch (e) {
      print('âŒ ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: $e');
    }
    return null;
  }

  /// ìŒì„± ë¶„ì„
  Future<ModalityData?> _analyzeAudio(String base64Audio) async {
    try {
      print('ğŸ¤ ìŒì„± ë¶„ì„ ì‹œì‘');
      final response = await _emotionApiService.sendAudioForAnalysis(base64Audio);
      
      if (response.containsKey('audio_emotion') && response.containsKey('audio_vad')) {
        final vad = response['audio_vad'] as Map<String, dynamic>;
        return ModalityData(
          valence: vad['valence']?.toDouble(),
          arousal: vad['arousal']?.toDouble(),
          dominance: vad['dominance']?.toDouble(),
          emotion: response['audio_emotion'],
          confidence: response['audio_confidence']?.toDouble() ?? 0.7,
          rawData: base64Audio,
        );
      }
    } catch (e) {
      print('âŒ ìŒì„± ë¶„ì„ ì‹¤íŒ¨: $e');
    }
    return null;
  }

  /// í…ìŠ¤íŠ¸ ë¶„ì„
  Future<ModalityData?> _analyzeText(String text) async {
    try {
      print('ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹œì‘');
      final response = await _emotionApiService.sendTextForAnalysis(text);
      
      if (response.containsKey('text_emotion') && response.containsKey('text_vad')) {
        final vad = response['text_vad'] as Map<String, dynamic>;
        return ModalityData(
          valence: vad['valence']?.toDouble(),
          arousal: vad['arousal']?.toDouble(),
          dominance: vad['dominance']?.toDouble(),
          emotion: response['text_emotion'],
          confidence: response['text_confidence']?.toDouble() ?? 0.6,
          rawData: text,
        );
      }
    } catch (e) {
      print('âŒ í…ìŠ¤íŠ¸ ë¶„ì„ ì‹¤íŒ¨: $e');
    }
    return null;
  }

  /// ì„œë²„ì— ì§ì ‘ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ìš”ì²­ (ì„œë²„ì—ì„œ í†µí•© ë¶„ì„ ì§€ì› ì‹œ)
  Future<MultimodalDataPoint?> _analyzeMultimodalOnServer({
    String? base64Image,
    String? base64Audio,
    String? text,
    String? sessionId,
    Map<String, dynamic>? metadata,
  }) async {
    try {
      print('ğŸŒ ì„œë²„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ìš”ì²­');
      
      final requestBody = {
        "face_image": base64Image ?? "",
        "audio": base64Audio ?? "",
        "text": text ?? "",
        if (sessionId != null) "session_id": sessionId,
        if (metadata != null) "metadata": metadata,
      };

      final response = await http.post(
        Uri.parse('$_baseUrl/analyze_multimodal_emotion'),
        headers: {"Content-Type": "application/json"},
        body: jsonEncode(requestBody),
      ).timeout(const Duration(seconds: 20));

      if (response.statusCode == 200) {
        final responseData = jsonDecode(response.body);
        return _parseServerMultimodalResponse(responseData, sessionId, metadata);
      } else {
        print('âŒ ì„œë²„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì‹¤íŒ¨: ${response.statusCode}');
        return null;
      }
    } catch (e) {
      print('âŒ ì„œë²„ ë©€í‹°ëª¨ë‹¬ ë¶„ì„ ì˜¤ë¥˜: $e');
      return null;
    }
  }

  /// ì„œë²„ ì‘ë‹µì„ MultimodalDataPointë¡œ íŒŒì‹±
  MultimodalDataPoint? _parseServerMultimodalResponse(
    Map<String, dynamic> response,
    String? sessionId,
    Map<String, dynamic>? metadata,
  ) {
    try {
      ModalityData? visualData;
      ModalityData? audioData;
      ModalityData? textData;

      // ì˜ìƒ ë°ì´í„° íŒŒì‹±
      if (response.containsKey('visual_analysis')) {
        final visual = response['visual_analysis'] as Map<String, dynamic>;
        visualData = ModalityData(
          valence: visual['valence']?.toDouble(),
          arousal: visual['arousal']?.toDouble(),
          dominance: visual['dominance']?.toDouble(),
          emotion: visual['emotion'],
          confidence: visual['confidence']?.toDouble(),
        );
      }

      // ìŒì„± ë°ì´í„° íŒŒì‹±
      if (response.containsKey('audio_analysis')) {
        final audio = response['audio_analysis'] as Map<String, dynamic>;
        audioData = ModalityData(
          valence: audio['valence']?.toDouble(),
          arousal: audio['arousal']?.toDouble(),
          dominance: audio['dominance']?.toDouble(),
          emotion: audio['emotion'],
          confidence: audio['confidence']?.toDouble(),
        );
      }

      // í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì‹±
      if (response.containsKey('text_analysis')) {
        final text = response['text_analysis'] as Map<String, dynamic>;
        textData = ModalityData(
          valence: text['valence']?.toDouble(),
          arousal: text['arousal']?.toDouble(),
          dominance: text['dominance']?.toDouble(),
          emotion: text['emotion'],
          confidence: text['confidence']?.toDouble(),
        );
      }

      // í†µí•© ê²°ê³¼ íŒŒì‹±
      final finalVad = response['final_vad'] as Map<String, dynamic>;
      
      return MultimodalDataPoint(
        timestamp: DateTime.now(),
        visualData: visualData,
        audioData: audioData,
        textData: textData,
        finalValence: finalVad['valence']?.toDouble() ?? 0.0,
        finalArousal: finalVad['arousal']?.toDouble() ?? 0.0,
        finalDominance: finalVad['dominance']?.toDouble() ?? 0.0,
        finalEmotion: response['final_emotion'] ?? 'neutral',
        finalConfidence: response['final_confidence']?.toDouble() ?? 0.0,
        sessionId: sessionId,
        metadata: metadata,
      );
    } catch (e) {
      print('âŒ ì„œë²„ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: $e');
      return null;
    }
  }

  /// ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë©€í‹°ëª¨ë‹¬ ë¶„ì„ (ì„¸ì…˜ ì¤‘ ì§€ì†ì  ë¶„ì„)
  Stream<MultimodalDataPoint> streamMultimodalAnalysis({
    required Stream<String?> imageStream,
    required Stream<String?> audioStream,
    required Stream<String?> textStream,
    String? sessionId,
    Duration interval = const Duration(seconds: 2),
  }) {
    return Stream.periodic(interval).asyncMap((_) async {
      // ê° ìŠ¤íŠ¸ë¦¼ì˜ ìµœì‹  ê°’ë“¤ì„ ìˆ˜ì§‘
      String? latestImage;
      String? latestAudio;
      String? latestText;

      // ìµœì‹  ê°’ë“¤ì„ ê°€ì ¸ì˜¤ëŠ” ë¡œì§ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ ë°©ë²• ì‚¬ìš©)
      
      return await analyzeMultimodal(
        base64Image: latestImage,
        base64Audio: latestAudio,
        text: latestText,
        sessionId: sessionId,
      );
    });
  }

  /// Mock ë©€í‹°ëª¨ë‹¬ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
  MultimodalDataPoint generateMockMultimodalData({
    String? sessionId,
    Map<String, dynamic>? metadata,
  }) {
    return MultimodalDataPoint.mock();
  }

  /// ë¶„ì„ í’ˆì§ˆ í‰ê°€
  double evaluateAnalysisQuality(MultimodalDataPoint dataPoint) {
    double quality = 0.0;
    int modalityCount = 0;

    // ê° ëª¨ë‹¬ë¦¬í‹°ë³„ í’ˆì§ˆ í‰ê°€
    if (dataPoint.visualData != null) {
      quality += dataPoint.visualData!.confidence ?? 0.5;
      modalityCount++;
    }
    if (dataPoint.audioData != null) {
      quality += dataPoint.audioData!.confidence ?? 0.5;
      modalityCount++;
    }
    if (dataPoint.textData != null) {
      quality += dataPoint.textData!.confidence ?? 0.5;
      modalityCount++;
    }

    // ëª¨ë‹¬ë¦¬í‹° ê°œìˆ˜ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
    if (modalityCount > 1) {
      quality += (modalityCount - 1) * 0.1; // ì¶”ê°€ ëª¨ë‹¬ë¦¬í‹°ë‹¹ 10% ë³´ë„ˆìŠ¤
    }

    return modalityCount > 0 ? quality / modalityCount : 0.0;
  }

  /// ë¶„ì„ ê²°ê³¼ ìš”ì•½
  String generateAnalysisSummary(MultimodalDataPoint dataPoint) {
    final quality = evaluateAnalysisQuality(dataPoint);
    final modalities = dataPoint.availableModalities;
    
    return '''
ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ì„ ê²°ê³¼:
- ì‚¬ìš©ëœ ëª¨ë‹¬ë¦¬í‹°: $modalitiesê°œ
- ìµœì¢… ê°ì •: ${dataPoint.finalEmotion}
- ì‹ ë¢°ë„: ${(dataPoint.finalConfidence * 100).toStringAsFixed(1)}%
- ë¶„ì„ í’ˆì§ˆ: ${(quality * 100).toStringAsFixed(1)}%
- VAD: (${dataPoint.finalValence.toStringAsFixed(2)}, ${dataPoint.finalArousal.toStringAsFixed(2)}, ${dataPoint.finalDominance.toStringAsFixed(2)})
''';
  }
} 